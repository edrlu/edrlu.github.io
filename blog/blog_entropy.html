<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross Entropy: A Brief Mathematical Breakdown</title>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <script src="../js/tex-mml-chtml.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fafafa;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: white;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            color: #2c3e50;
            font-weight: 300;
        }

        .meta {
            color: #7f8c8d;
            margin-bottom: 30px;
            font-size: 0.9em;
        }

        h2 {
            font-size: 1.8em;
            margin: 40px 0 20px 0;
            color: #34495e;
            font-weight: 400;
        }

        h3 {
            font-size: 1.4em;
            margin: 30px 0 15px 0;
            color: #34495e;
            font-weight: 400;
        }

        p {
            margin-bottom: 20px;
            text-align: justify;
        }

        .code-block {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 20px;
            margin: 25px 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            line-height: 1.4;
        }

        .code-block pre {
            margin: 0;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        .inline-code {
            background-color: #f1f3f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            color: #d63384;
        }

        .equation {
            background-color: #f8f9fa;
            padding: 20px;
            margin: 25px 0;
            text-align: center;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }

        .highlight {
            background-color: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
            border-radius: 4px;
        }

        .definition-box {
            background-color: #e8f4fd;
            padding: 20px;
            border-left: 4px solid #2196F3;
            margin: 20px 0;
            border-radius: 4px;
        }

        .example-box {
            background-color: #f0f8e8;
            padding: 15px;
            border-left: 4px solid #4CAF50;
            margin: 20px 0;
            border-radius: 4px;
        }

        .key-points {
            background-color: #fdf2e8;
            padding: 15px;
            border-left: 4px solid #FF9800;
            margin: 20px 0;
            border-radius: 4px;
        }

        .key-points ul {
            margin-left: 20px;
            margin-top: 10px;
        }

        .key-points li {
            margin-bottom: 8px;
        }

        .author {
            margin-top: 50px;
            padding-top: 30px;
            border-top: 1px solid #eee;
            color: #7f8c8d;
            font-style: italic;
        }

        .plot-placeholder {
            background-color: #f8f9fa;
            border: 2px dashed #dee2e6;
            padding: 40px;
            text-align: center;
            color: #6c757d;
            margin: 25px 0;
            border-radius: 4px;
        }

        @media (max-width: 600px) {
            .container {
                padding: 20px 15px;
            }
            
            h1 {
                font-size: 2em;
            }
            
            h2 {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Cross Entropy: A Brief Mathematical Breakdown</h1>
        <div class="meta">Published on August 28th, 2025 | 10 min read</div>
        
        <h2>What is Entropy?</h2>
        
        <p>In simple terms, entropy is the measure of surprise. The more surprising an event, the higher its entropy, and vice versa. Skewed distributions have lower entropy, while a uniform distribution (all outcomes equally likely) has the maximum entropy.</p>

        <div class="definition-box">
            <strong>Discrete Definition of Entropy:</strong><br>
            For a random variable \(x\) that takes value from a finite set \(X\) with a probability distribution, we have:
            <div class="equation">
                $$H(x) = -\sum_{x \in X} P(x) \log P(x)$$
            </div>
        </div>

        <div class="key-points">
            <strong>Note: The base of the logarithm is either e or 2. The base determines the unit in which we measure information.</strong>
            <ul>
                <li><strong>Base 2:</strong> Entropy is measured in bits. A bit represents the amount of information gained when distinguishing between two equally likely outcomes.</li>
                <li><strong>Base e:</strong> Entropy is measured in nats. A nat is the natural unit of information. 1 nat = \(\log_2(e) \approx 1.4427\) bits</li>
            </ul>
        </div>

        <div class="example-box">
            <strong>Example - Coin Flip:</strong><br>
            For a coin flip \(P(H) = P(T) = 0.5\)<br>
            In other words you need 1 yes/no question to fully determine the outcome.
        </div>

        <h3>Why Different Units?</h3>
        
        <p><strong>Base 2</strong> is commonly used in computer science and information theory because of binary decisions, while <strong>Base e</strong> is common in probability theory, machine learning, and statistics.</p>

        <h3>Breaking down the definition:</h3>

        <p>- \(\log(P(x))\): is the information content of outcome \(x\).</p>
        <p>- \(\log\) is used due to \(0 < P(x) \leq 1\). \(\log(0 < x \leq 1)\) is negative thus we add a negative sign to negate that feature of log. \(I(x) > 0\)</p>

        <p><strong>Why log?</strong> log allows rarer events to carry more information. For example: \(-\log(0.5)=1\) versus \(-\log(0.01) \approx 6.64\)</p>

        <p>So why log instead of \(1/P(x)\). Well log is mathematically nicer:</p>
        <p>Ex: \(I(x, y) = I(x)+I(y)\) because of the log addition property</p>

        <p>Combining \(I(x)\) with \(P(x)\) gives us \(P(x)I(x)\). In other words entropy is a weighted expected self-information:</p>

        <div class="definition-box">
            <strong>Entropy as Expectation</strong>
        </div>

        <div class="key-points">
            <strong>Range of entropy:</strong>
            <ul>
                <li><strong>Min:</strong> \(H(x) = 0\)</li>
                <li><strong>Max:</strong> \(H(x) = \log(n)\)</li>
            </ul>
        </div>

        <h2>Cross Entropy: Adding in another distribution</h2>

        <p>Given true distribution \(P(x)\) and predicted distribution \(Q(x)\), cross entropy can be written as:</p>

        <div class="equation">
            $$H(P,Q) = -\sum_{x} P(x) \log Q(x)$$
        </div>

        <p>Cross-entropy measures how well the predicted distribution \(Q\) matches the true distribution \(P\).</p>

        <h3>Implementation Example</h3>

        <div class="code-block">
<pre>import numpy as np

def cross_entropy(P, Q, base=np.e):
    epsilon = 1e-15
    Q = np.clip(Q, epsilon, 1. - epsilon)
    return -np.sum(P * np.log(Q)) / np.log(base)

# True distribution (one-hot label)
P = np.array([1, 0, 0])
# Predicted distribution from the model
Q = np.array([0.7, 0.2, 0.1])

# Compute in nats (natural log)
ce_nats = cross_entropy(P, Q, base=np.e)
print(f"Cross-entropy (nats): {ce_nats:.3f}")

# Output: Cross-entropy (nats): 0.357</pre>
        </div>

        <h2>Our first loss function: Log Loss</h2>

        <p>Log Loss (aka Binary Cross-Entropy) is when the true label is denoted as \(y_i \in \{0, 1\}\) and predicted probability is \(\hat{y}_i \in (0, 1)\). The true value label is either 0 or 1, (example: spam, not spam) and predicted value is between 0 and 1.</p>

        <h3>Visualizing Log Loss</h3>

        <div class="code-block">
<pre>import numpy as np
import matplotlib.pyplot as plt

def binary_log_loss(y_true, y_pred, eps=1e-15):
    """
    Compute binary log loss for arrays of predictions
    """
    y_pred = np.clip(y_pred, eps, 1 - eps)  # avoid log(0)
    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Range of predicted probabilities (from almost 0 to almost 1)
pred_probs = np.linspace(0.001, 0.999, 500)

# Log loss curves for y=1 and y=0
loss_true1 = binary_log_loss(1, pred_probs)
loss_true0 = binary_log_loss(0, pred_probs)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(pred_probs, loss_true1, label="True label = 1", color='blue')
plt.plot(pred_probs, loss_true0, label="True label = 0", color='red')
plt.title("Binary Log Loss vs Predicted Probability")
plt.xlabel("Predicted Probability for Class 1")
plt.ylabel("Log Loss")
plt.legend()
plt.grid(True)
plt.show()</pre>
        </div>

        <div class="plot-placeholder">
            <img src="../images/blog/log_loss.png" alt="Binary Log Loss Plot">
        </div>

        <div class="highlight">
            <strong>Key Insight:</strong> Log Loss punishes bigger mispredictions exponentially. When you're very confident but wrong (predicting 0.95 when true label is 0), the loss becomes very high. This encourages the model to be cautious about overconfident predictions.
        </div>

        <h2>Why Cross Entropy Works So Well</h2>

        <p>Cross entropy is particularly effective as a loss function for several reasons:</p>

        <div class="key-points">
            <ul>
                <li><strong>Probabilistic Interpretation:</strong> It directly measures how well predicted probabilities match true distributions</li>
                <li><strong>Gradient Properties:</strong> Has nice gradient properties that facilitate optimization</li>
                <li><strong>Penalty Structure:</strong> Heavily penalizes confident wrong predictions while being gentle on uncertain predictions</li>
                <li><strong>Information Theoretic Foundation:</strong> Rooted in information theory, providing a principled approach to measuring prediction quality</li>
            </ul>
        </div>

        <h2>Practical Applications</h2>

        <p>Cross entropy finds extensive use in machine learning:</p>

        <p><strong>Binary Classification:</strong> Binary cross-entropy for problems with two classes (spam detection, medical diagnosis).</p>

        <p><strong>Multi-class Classification:</strong> Categorical cross-entropy for problems with multiple mutually exclusive classes (image classification, sentiment analysis).</p>

        <p><strong>Neural Networks:</strong> Standard loss function for training neural networks in classification tasks.</p>

        <p><strong>Natural Language Processing:</strong> Used in language models to predict next words or tokens.</p>

        <h2>Conclusion</h2>

        <p>Cross entropy bridges information theory and machine learning, providing both theoretical foundation and practical effectiveness. Understanding its mathematical underpinnings helps in making informed decisions about model architecture, interpreting training dynamics, and debugging classification problems.</p>

        <p>The beauty of cross entropy lies in its simplicity and effectiveness - it elegantly captures the intuition that confident wrong predictions should be penalized more heavily than uncertain ones, while providing the mathematical rigor needed for optimization algorithms.</p>

        <div class="author">
            Based on original content by the author | Enhanced for educational purposes
        </div>
    </div>
</body>
</html>
